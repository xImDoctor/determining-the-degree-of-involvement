import streamlit as st
import os
import tempfile
import cv2
import numpy as np
from PIL import Image
import time
import threading
import queue
import atexit
from pathlib import Path
import sys
import subprocess
from collections import deque
from time import time as current_time

sys.path.append('face_detection_and_emotion_recognition.py')


try:
    from face_detection_and_emotion_recognition import (
        FaceDetector,
        EmotionRecognizer,
        DetectFaceAndRecognizeEmotion,
        process_video_stream,
        CaptureReadError
    )

    BACKEND_AVAILABLE = True
except ImportError as e:
    st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥—É–ª—å –±—ç–∫–µ–Ω–¥–∞: {e}")
    BACKEND_AVAILABLE = False

APP_TITLE = "Real-time Emotion Detection"
APP_ICON = "üé≠"
SUPPORTED_FORMATS = ['mp4', 'avi', 'mov', 'mkv', 'webm', 'wmv']
MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB


# ============================================
# CSS –°–¢–ò–õ–ò
# ============================================

st.set_page_config(
    page_title=APP_TITLE,
    page_icon=APP_ICON,
    layout="wide",
    initial_sidebar_state="expanded"
)

st.markdown("""
<style>
    /* –û—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç–∏–ª–∏ */
    .main-header {
        font-size: 2.8rem;
        text-align: center;
        margin-bottom: 1rem;
        font-weight: 800;
        padding: 0.5rem;
    }

    .webcam-container {
        border: 3px solid #667eea;
        border-radius: 15px;
        padding: 1rem;
        background: linear-gradient(135deg, #667eea0a 0%, #764ba20a 100%);
        margin: 2rem 0;
    }

    .processing-card {
        background: linear-gradient(135deg, #667eea, #764ba2);
        color: white;
        border-radius: 15px;
        padding: 2rem;
        margin: 2rem 0;
        box-shadow: 0 10px 30px rgba(102, 126, 234, 0.3);
    }

    .result-card {
        background: linear-gradient(135deg, #56ab2f, #a8e063);
        color: white;
        border-radius: 15px;
        padding: 2rem;
        margin: 2rem 0;
        box-shadow: 0 10px 30px rgba(86, 171, 47, 0.3);
    }

    .error-card {
        background: linear-gradient(135deg, #ff416c, #ff4b2b);
        color: white;
        border-radius: 15px;
        padding: 2rem;
        margin: 2rem 0;
        box-shadow: 0 10px 30px rgba(255, 65, 108, 0.3);
    }

    .video-container {
        border-radius: 15px;
        overflow: hidden;
        box-shadow: 0 15px 35px rgba(0,0,0,0.1);
        background: #000;
    }

    .emotion-display {
        display: flex;
        flex-wrap: wrap;
        gap: 10px;
        margin-top: 15px;
    }

    .emotion-item {
        background: linear-gradient(135deg, #667eea, #764ba2);
        color: white;
        padding: 8px 15px;
        border-radius: 20px;
        font-size: 0.9rem;
        font-weight: 500;
    }

    .emotion-confidence {
        font-size: 0.8rem;
        opacity: 0.9;
    }
</style>
""", unsafe_allow_html=True)


# ============================================
# –ö–õ–ê–°–°–´ –î–õ–Ø –û–ë–†–ê–ë–û–¢–ö–ò –í–ò–î–ï–û
# ============================================

class EmotionDetectionProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º DetectFaceAndRecognizeEmotion"""

    def __init__(self):
        self.detector = None
        self.is_initialized = False
        self.current_emotions = []

    def initialize_models(self, params):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —ç–º–æ—Ü–∏–π"""
        try:
            if BACKEND_AVAILABLE:
                #–¥–µ—Ç–µ–∫—Ç–æ—Ä –ª–∏—Ü
                face_detector = FaceDetector(
                    min_detection_confidence=params.get('min_detection_confidence', 0.5)
                )

                #=—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å —ç–º–æ—Ü–∏–π
                emotion_recognizer = EmotionRecognizer(
                    window_size=params.get('window_size', 15),
                    confidence_threshold=params.get('confidence_threshold', 0.55),
                    ambiguity_threshold=params.get('ambiguity_threshold', 0.15)
                )

                #=–æ—Å–Ω–æ–≤–Ω–æ–π –¥–µ—Ç–µ–∫—Ç–æ—Ä
                self.detector = DetectFaceAndRecognizeEmotion(face_detector, emotion_recognizer)

                self.is_initialized = True
                return True, "–ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã"
            else:
                return False, "–ú–æ–¥—É–ª—å –±—ç–∫–µ–Ω–¥–∞ –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω"

        except Exception as e:
            return False, f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π: {str(e)}"

    def process_frame(self, frame, flip_h=False):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –∫–∞–¥—Ä –≤–∏–¥–µ–æ"""
        if not self.is_initialized or self.detector is None:
            return frame, []

        try:
            # –û—Ç–∑–µ—Ä–∫–∞–ª–∏–≤–∞–Ω–∏–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if flip_h:
                frame = cv2.flip(frame, 1)

            # –û–±—Ä–∞–±–∞—Ç–∫–∞ –∫–∞–¥—Ä–∞ —Å –ø–æ–º–æ—â—å—é –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞
            processed_frame, emotions = self.detector.detect_and_recognize(frame)

            self.current_emotions = emotions
            return processed_frame, emotions

        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–¥—Ä–∞: {e}")
            return frame, []

    def get_emotion_statistics(self):
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–º —ç–º–æ—Ü–∏—è–º"""
        if not self.current_emotions:
            return {}

        stats = {}
        for emotion, confidence in self.current_emotions:
            if emotion in stats:
                stats[emotion] += 1
            else:
                stats[emotion] = 1

        return stats

    def reset(self):
        """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞"""
        self.current_emotions = []
        if hasattr(self.detector, 'face_detector'):
            if hasattr(self.detector.face_detector, 'close'):
                self.detector.face_detector.close()
        if hasattr(self.detector, 'emotion_recognizer'):
            if hasattr(self.detector.emotion_recognizer, 'reset'):
                self.detector.emotion_recognizer.reset()


class VideoFileProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ—Ñ–∞–π–ª–æ–≤"""

    def __init__(self):
        self.detection_processor = EmotionDetectionProcessor()

    def extract_video_info(self, video_path):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–∏–¥–µ–æ"""
        try:
            cap = cv2.VideoCapture(video_path)

            if not cap.isOpened():
                return {"error": "Cannot open video file"}

            info = {
                "width": int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),
                "height": int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),
                "fps": int(cap.get(cv2.CAP_PROP_FPS)),
                "frame_count": int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),
                "duration": round(cap.get(cv2.CAP_PROP_FRAME_COUNT) / cap.get(cv2.CAP_PROP_FPS), 2) if cap.get(
                    cv2.CAP_PROP_FPS) > 0 else 0,
                "format": self._get_video_format(video_path)
            }

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–µ–≤—å—é
            ret, frame = cap.read()
            if ret:
                preview_path = "temp_preview.jpg"
                cv2.imwrite(preview_path, frame)
                info["preview"] = preview_path

            cap.release()
            return info

        except Exception as e:
            return {"error": f"Cannot extract video info: {str(e)}"}

    def _get_video_format(self, video_path):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ñ–æ—Ä–º–∞—Ç –≤–∏–¥–µ–æ"""
        ext = os.path.splitext(video_path)[1].lower()
        formats = {
            '.mp4': 'MP4',
            '.avi': 'AVI',
            '.mov': 'MOV',
            '.mkv': 'MKV',
            '.webm': 'WebM',
            '.wmv': 'WMV'
        }
        return formats.get(ext, 'Unknown')

    def process_video_file(self, input_path, output_path, params, progress_callback=None):
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∏–¥–µ–æ—Ñ–∞–π–ª —Å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ–º —ç–º–æ—Ü–∏–π

        Args:
            input_path: –ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É –≤–∏–¥–µ–æ
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏
            progress_callback: –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞

        Returns:
            (success, message, output_path, statistics)
        """
        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏
            success, message = self.detection_processor.initialize_models(params)
            if not success:
                return False, message, None, {}

            # –û—Ç–∫—Ä—ã–≤–∞–µ–º –≤–∏–¥–µ–æ
            cap = cv2.VideoCapture(input_path)
            if not cap.isOpened():
                return False, f"Cannot open video file: {input_path}", None, {}

            # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–∏–¥–µ–æ
            frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            fps = int(cap.get(cv2.CAP_PROP_FPS))
            if fps == 0:
                fps = 30

            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

            # –°–æ–∑–¥–∞–µ–º VideoWriter –¥–ª—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –≤–∏–¥–µ–æ
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            all_emotions = []
            frame_count = 0

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º  –∫–∞–¥—Ä
            while True:
                ret, frame = cap.read()
                if not ret:
                    break

                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–¥—Ä
                processed_frame, emotions = self.detection_processor.process_frame(
                    frame,
                    flip_h=params.get('flip_h', False)
                )

                # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∫–∞–¥—Ä
                out.write(processed_frame)

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–æ—Ü–∏–∏ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                for emotion, confidence in emotions:
                    all_emotions.append(emotion)

                frame_count += 1

                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                if progress_callback and total_frames > 0:
                    progress = frame_count / total_frames
                    progress_callback(progress, frame_count, total_frames, emotions)

            # –ó–∞–∫—Ä—ã–≤–∞–µ–º –≤–∏–¥–µ–æ
            cap.release()
            out.release()

            # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            statistics = self._calculate_statistics(all_emotions, frame_count)

            return True, "–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ", output_path, statistics

        except Exception as e:
            return False, f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ: {str(e)}", None, {}

    def _calculate_statistics(self, all_emotions, total_frames):
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —ç–º–æ—Ü–∏—è–º"""
        if not all_emotions:
            return {}

        stats = {}
        for emotion in all_emotions:
            if emotion in stats:
                stats[emotion] += 1
            else:
                stats[emotion] = 1

        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç—ã
        total_detections = len(all_emotions)
        if total_detections > 0:
            for emotion in stats:
                stats[f"{emotion}_percent"] = (stats[emotion] / total_detections) * 100

        stats['total_frames'] = total_frames
        stats['total_detections'] = total_detections
        stats['detection_rate'] = (total_detections / total_frames) * 100 if total_frames > 0 else 0

        return stats

    def extract_sample_frames(self, video_path, num_frames=4):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–∞–¥—Ä–æ–≤ –∏–∑ –≤–∏–¥–µ–æ –¥–ª—è –ø—Ä–µ–≤—å—é"""
        try:
            cap = cv2.VideoCapture(video_path)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

            if total_frames == 0:
                return []

            # –í—ã–±–∏—Ä–∞–µ–º —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –∫–∞–¥—Ä—ã
            frame_indices = np.linspace(0, total_frames - 1, min(num_frames, total_frames), dtype=int)

            frames = []
            for idx in frame_indices:
                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
                ret, frame = cap.read()
                if ret:
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º BGR –≤ RGB
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    frames.append(frame_rgb)

            cap.release()
            return frames

        except Exception as e:
            return []


# ============================================
# –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –°–ï–°–°–ò–ò
# ============================================

if 'video_processor' not in st.session_state:
    st.session_state.video_processor = VideoFileProcessor()

if 'detection_processor' not in st.session_state:
    st.session_state.detection_processor = EmotionDetectionProcessor()

if 'uploaded_file_path' not in st.session_state:
    st.session_state.uploaded_file_path = None

if 'processing_status' not in st.session_state:
    st.session_state.processing_status = "idle"

if 'result_path' not in st.session_state:
    st.session_state.result_path = None

if 'video_info' not in st.session_state:
    st.session_state.video_info = {}

if 'processing_progress' not in st.session_state:
    st.session_state.processing_progress = 0

if 'current_emotions' not in st.session_state:
    st.session_state.current_emotions = []

if 'emotion_statistics' not in st.session_state:
    st.session_state.emotion_statistics = {}

if 'backend_params' not in st.session_state:
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    st.session_state.backend_params = {
        'min_detection_confidence': 0.5,
        'window_size': 15,
        'confidence_threshold': 0.55,
        'ambiguity_threshold': 0.15,
        'margin': 20,
        'flip_h': False,
        'show_preview': False
    }

if 'webcam_running' not in st.session_state:
    st.session_state.webcam_running = False


# ============================================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ============================================

def display_header():
    """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.markdown(f'<h1 class="main-header">{APP_TITLE}</h1>', unsafe_allow_html=True)
        st.markdown(
            '<p style="text-align: center; color: #666; font-size: 1.2rem; margin-bottom: 2rem;">Upload a video or use webcam to detect faces and recognize emotions in real-time</p>',
            unsafe_allow_html=True)


def display_sidebar():
    """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –±–æ–∫–æ–≤—É—é –ø–∞–Ω–µ–ª—å —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏"""
    with st.sidebar:
        st.markdown("### üé≠ Emotion Detection")

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ
        st.markdown("#### ‚ÑπÔ∏è System Status")

        if BACKEND_AVAILABLE:
            st.success("‚úÖ Backend module available")
        else:
            st.error("‚ùå Backend module not found")
            st.info("Please ensure face_detection_and_emotion_recognition.py is in the current directory")

        st.markdown("---")

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        st.markdown("#### ‚öôÔ∏è Processing Parameters")

        # Face Detector Parameters
        st.markdown("##### Face Detection")
        st.session_state.backend_params['min_detection_confidence'] = st.slider(
            "Min Detection Confidence",
            min_value=0.0,
            max_value=1.0,
            value=st.session_state.backend_params['min_detection_confidence'],
            step=0.01,
            help="–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –ª–∏—Ü"
        )

        # Emotion Recognizer Parameters
        st.markdown("##### Emotion Recognition")

        st.session_state.backend_params['window_size'] = st.slider(
            "Window Size",
            min_value=3,
            max_value=30,
            value=st.session_state.backend_params['window_size'],
            step=1,
            help="–†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –¥–ª—è —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è"
        )

        st.session_state.backend_params['confidence_threshold'] = st.slider(
            "Confidence Threshold",
            min_value=0.0,
            max_value=1.0,
            value=st.session_state.backend_params['confidence_threshold'],
            step=0.01,
            help="–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è —ç–º–æ—Ü–∏–∏"
        )

        st.session_state.backend_params['ambiguity_threshold'] = st.slider(
            "Ambiguity Threshold",
            min_value=0.0,
            max_value=1.0,
            value=st.session_state.backend_params['ambiguity_threshold'],
            step=0.01,
            help="–ü–æ—Ä–æ–≥ –¥–ª—è –∞–º–±–∏–≤–∞–ª–µ–Ω—Ç–Ω—ã—Ö —ç–º–æ—Ü–∏–π"
        )

        # –û–±—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        st.markdown("##### General Settings")
        st.session_state.backend_params['flip_h'] = st.checkbox(
            "Flip Horizontal",
            value=st.session_state.backend_params['flip_h'],
            help="–û—Ç–∑–µ—Ä–∫–∞–ª–∏–≤–∞–Ω–∏–µ –ø–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª–∏"
        )

        st.session_state.backend_params['show_preview'] = st.checkbox(
            "Show Preview",
            value=st.session_state.backend_params['show_preview'],
            help="–ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –ø—Ä–µ–≤—å—é –≤–æ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"
        )

        st.markdown("---")

        # –ö–Ω–æ–ø–∫–∞ —Å–±—Ä–æ—Å–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if st.button("üîÑ Reset to Default", use_container_width=True):
            st.session_state.backend_params = {
                'min_detection_confidence': 0.5,
                'window_size': 15,
                'confidence_threshold': 0.55,
                'ambiguity_threshold': 0.15,
                'flip_h': False,
                'show_preview': False
            }
            st.rerun()


def create_upload_section():
    """–°–æ–∑–¥–∞–µ—Ç —Å–µ–∫—Ü–∏—é –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞"""
    st.markdown("### üì§ Upload Video")

    # –ó–æ–Ω–∞ –∑–∞–≥—Ä—É–∑–∫–∏
    uploaded_file = st.file_uploader(
        "Choose a video file",
        type=SUPPORTED_FORMATS,
        help=f"Supported formats: {', '.join(SUPPORTED_FORMATS).upper()}",
        label_visibility="collapsed"
    )

    if uploaded_file is not None:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
        if uploaded_file.size > MAX_FILE_SIZE:
            st.error(f"File too large! Maximum size is {MAX_FILE_SIZE // (1024 * 1024)}MB")
            return

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        temp_dir = tempfile.gettempdir()
        temp_path = os.path.join(temp_dir, uploaded_file.name)

        with open(temp_path, "wb") as f:
            f.write(uploaded_file.getvalue())

        st.session_state.uploaded_file_path = temp_path

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–∏–¥–µ–æ
        video_info = st.session_state.video_processor.extract_video_info(temp_path)
        st.session_state.video_info = video_info

        if "error" not in video_info:
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∞–π–ª–µ
            display_file_info(uploaded_file, video_info)

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–≤—å—é –≤–∏–¥–µ–æ
            display_video_preview(temp_path, video_info)

            # –ö–Ω–æ–ø–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            col1, col2 = st.columns(2)
            with col1:
                if st.button("üöÄ Start Emotion Detection", type="primary", use_container_width=True):
                    if not BACKEND_AVAILABLE:
                        st.error("–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –Ω–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É: Backend –º–æ–¥—É–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª face_detection_and_emotion_recognition.py –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å–æ –≤—Å–µ–º–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏.")
                    else:
                        st.session_state.processing_status = "starting"
                        st.rerun()
            with col2:
                if st.button("üóëÔ∏è Clear File", use_container_width=True):
                    st.session_state.uploaded_file_path = None
                    st.rerun()
        else:
            st.error(f"Error: {video_info['error']}")

    else:
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–¥—Å–∫–∞–∑–∫—É
        st.info("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –≤–∏–¥–µ–æ—Ñ–∞–π–ª, –∏—Å–ø–æ–ª—å–∑—É—è –∑–∞–≥—Ä—É–∑—á–∏–∫ –≤—ã—à–µ.")
        st.markdown("""
        **–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã:** MP4, AVI, MOV, MKV, WebM, WMV
        **–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä:** 100MB
        """)


def display_file_info(uploaded_file, video_info):
    """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∞–π–ª–µ"""
    st.markdown("### üìä Video Information")

    # –ù–∞—Ç–∏–≤–Ω—ã–µ Streamlit metrics –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤–∏–¥–µ–æ—Ñ–∞–π–ª–µ
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric(
            label="Resolution",
            value=f"{video_info['width']}√ó{video_info['height']}"
        )

    with col2:
        duration = video_info["duration"]
        if duration >= 60:
            minutes = int(duration // 60)
            seconds = int(duration % 60)
            duration_str = f"{minutes}:{seconds:02d}"
        else:
            duration_str = f"{duration:.1f}s"

        st.metric(
            label="Duration",
            value=duration_str
        )

    with col3:
        st.metric(
            label="FPS",
            value=video_info["fps"]
        )

    with col4:
        file_size_mb = uploaded_file.size / (1024 * 1024)
        st.metric(
            label="Size (MB)",
            value=f"{file_size_mb:.1f}"
        )


def display_video_preview(video_path, video_info):
    """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –ø—Ä–µ–≤—å—é –≤–∏–¥–µ–æ"""
    st.markdown("### üëÄ Video Preview")

    # –û—Å–Ω–æ–≤–Ω–æ–µ –≤–∏–¥–µ–æ
    st.markdown('<div class="video-container">', unsafe_allow_html=True)
    video_bytes = open(video_path, "rb").read()
    st.video(video_bytes)
    st.markdown('</div>', unsafe_allow_html=True)

    # –ü—Ä–∏–º–µ—Ä—ã –∫–∞–¥—Ä–æ–≤
    st.markdown("#### üì∏ Sample Frames")

    frames = st.session_state.video_processor.extract_sample_frames(video_path, 4)
    if frames:
        cols = st.columns(4)
        for idx, (col, frame) in enumerate(zip(cols, frames)):
            with col:
                img = Image.fromarray(frame)
                st.image(img, caption=f"Frame {idx + 1}", use_container_width=True)


def process_video():
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∏–¥–µ–æ"""
    if st.session_state.processing_status == "starting" and st.session_state.uploaded_file_path:
        st.session_state.processing_status = "processing"

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–∞–Ω–µ–ª—å –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        st.markdown('<div class="processing-card">', unsafe_allow_html=True)
        st.markdown("### ‚öôÔ∏è Processing Your Video")

        progress_bar = st.progress(0)
        status_text = st.empty()
        stats_text = st.empty()

        # –§—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        def update_progress(progress, current_frame, total_frames, emotions):
            progress_bar.progress(progress)
            status_text.text(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–¥—Ä–∞ {current_frame} –∏–∑ {total_frames} ({progress * 100:.1f}%)")

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            if emotions:
                emotion_stats = {}
                for emotion, confidence in emotions:
                    emotion_stats[emotion] = emotion_stats.get(emotion, 0) + 1

                stats_text.markdown("**–¢–µ–∫—É—â–∏–µ —ç–º–æ—Ü–∏–∏:** " + ", ".join([f"{k}: {v}" for k, v in emotion_stats.items()]))

        # –°–æ–∑–¥–∞–µ–º –∏–º—è –¥–ª—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        input_path = st.session_state.uploaded_file_path
        input_name = os.path.splitext(os.path.basename(input_path))[0]
        timestamp = int(current_time())
        output_filename = f"emotion_detected_{input_name}_{timestamp}.mp4"
        output_path = os.path.join(tempfile.gettempdir(), output_filename)

        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
        success, message, result_path, statistics = st.session_state.video_processor.process_video_file(
            input_path,
            output_path,
            st.session_state.backend_params,
            update_progress
        )

        if success:
            st.session_state.processing_status = "completed"
            st.session_state.result_path = result_path
            st.session_state.emotion_statistics = statistics
        else:
            st.session_state.processing_status = "failed"
            st.session_state.error_message = message

        st.markdown('</div>', unsafe_allow_html=True)

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if st.session_state.processing_status == "completed":
            display_result()
        elif st.session_state.processing_status == "failed":
            display_error()


def display_result():
    """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    st.markdown('<div class="result-card">', unsafe_allow_html=True)
    st.markdown("### ‚úÖ Processing Completed!")

    result_path = st.session_state.result_path

    if result_path and os.path.exists(result_path):
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –≤–∏–¥–µ–æ
        st.markdown("#### üé¨ Processed Video")
        st.markdown('<div class="video-container">', unsafe_allow_html=True)
        st.video(result_path)
        st.markdown('</div>', unsafe_allow_html=True)

        # —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–º–æ—Ü–∏–π
        if st.session_state.emotion_statistics:
            st.markdown("#### üìä Emotion Statistics")

            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —ç–º–æ—Ü–∏–∏ (–∏—Å–∫–ª—é—á–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø–æ–ª—è)
            emotion_stats = {k: v for k, v in st.session_state.emotion_statistics.items()
                             if not k.endswith('_percent') and k not in ['total_frames', 'total_detections',
                                                                         'detection_rate']}

            if emotion_stats:
                cols = st.columns(len(emotion_stats))
                for idx, (emotion, count) in enumerate(emotion_stats.items()):
                    with cols[idx % len(cols)]:
                        percent_key = f"{emotion}_percent"
                        percent = st.session_state.emotion_statistics.get(percent_key, 0)
                        st.metric(emotion.capitalize(), f"{count} ({percent:.1f}%)")

            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            st.markdown("#### üìà Overall Statistics")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Total Frames", st.session_state.emotion_statistics.get('total_frames', 0))
            with col2:
                st.metric("Face Detections", st.session_state.emotion_statistics.get('total_detections', 0))
            with col3:
                st.metric("Detection Rate", f"{st.session_state.emotion_statistics.get('detection_rate', 0):.1f}%")

        # –ö–Ω–æ–ø–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
        with open(result_path, "rb") as f:
            st.download_button(
                label="üì• Download Processed Video",
                data=f,
                file_name=os.path.basename(result_path),
                mime="video/mp4",
                type="primary",
                use_container_width=True
            )

    else:
        st.warning("Processed video file not found")

    # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –Ω–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
    col1, col2 = st.columns(2)
    with col1:
        if st.button("üîÑ Process Another Video", use_container_width=True):
            st.session_state.uploaded_file_path = None
            st.session_state.processing_status = "idle"
            st.session_state.result_path = None
            st.session_state.emotion_statistics = {}
            st.rerun()

    st.markdown('</div>', unsafe_allow_html=True)


def display_error():
    """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –æ—à–∏–±–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    st.markdown('<div class="error-card">', unsafe_allow_html=True)
    st.markdown("### ‚ùå Processing Failed")

    error_msg = getattr(st.session_state, 'error_message', 'Unknown error')
    st.error(f"Error: {error_msg}")

    # –°–æ–≤–µ—Ç—ã –ø–æ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—é –Ω–µ–ø–æ–ª–∞–¥–æ–∫
    st.markdown("#### üîß Troubleshooting Tips:")
    st.markdown("""
    1. ‚úÖ Ensure `face_detection_and_emotion_recognition.py` is in the same directory
    2. ‚úÖ Check if all dependencies are installed
    3. ‚úÖ Try a shorter video (under 1 minute)
    4. ‚úÖ Ensure the video format is supported
    5. ‚úÖ Check available disk space
    """)

    if st.button("üîÑ Try Again", use_container_width=True):
        st.session_state.processing_status = "idle"
        st.rerun()

    st.markdown('</div>', unsafe_allow_html=True)


def create_webcam_section():
    """–°–æ–∑–¥–∞–µ—Ç —Å–µ–∫—Ü–∏—é —Ä–∞–±–æ—Ç—ã —Å –≤–µ–±-–∫–∞–º–µ—Ä–æ–π"""
    st.markdown("### üì∑ Webcam Live Emotion Detection")

    if not BACKEND_AVAILABLE:
        st.warning(
            "Webcam emotion detection requires backend module. Please ensure face_detection_and_emotion_recognition.py is available.")
        return

    col1, col2 = st.columns([3, 1])

    with col1:
        # –ö–Ω–æ–ø–∫–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        control_col1, control_col2 = st.columns(2)

        with control_col1:
            start_webcam = st.button("üé¨ Start Webcam", type="primary", use_container_width=True)

        with control_col2:
            stop_webcam = st.button("‚èπÔ∏è Stop Webcam", type="secondary", use_container_width=True)

        # –ú–µ—Å—Ç–æ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤–∏–¥–µ–æ
        webcam_placeholder = st.empty()
        emotions_placeholder = st.empty()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats_placeholder = st.empty()
        fps_placeholder = st.empty()

        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –≤–µ–±-–∫–∞–º–µ—Ä—ã
        if start_webcam:
            st.session_state.webcam_running = True

        if stop_webcam:
            st.session_state.webcam_running = False

        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤–µ–±-–∫–∞–º–µ—Ä—É
        if st.session_state.get('webcam_running', False):
            cap = cv2.VideoCapture(0)

            if not cap.isOpened():
                st.error("Cannot open webcam")
                st.session_state.webcam_running = False
            else:
                # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞–º–µ—Ä—ã
                cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
                cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é process_video_stream –∏–∑ –±—ç–∫–µ–Ω–¥–∞
                try:
                    fps_history = deque(maxlen=3)
                    for _ in range(3):
                        fps_history.append(0.0)

                    emotion_history = []
                    start_time = current_time()

                    for img, emotions in process_video_stream(cap, flip_h=st.session_state.backend_params['flip_h']):
                        if not st.session_state.get('webcam_running', False):
                            break

                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–æ—Ü–∏–∏ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                        if emotions:
                            for emotion, confidence in emotions:
                                emotion_history.append(emotion)

                        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
                        if len(emotion_history) > 100:
                            emotion_history = emotion_history[-100:]

                        # –†–∞—Å—á–µ—Ç FPS
                        fps = 1 / (current_time() - start_time)
                        fps_history.append(fps)
                        avg_fps = round(sum(fps_history) / len(fps_history))

                        # –î–æ–±–∞–≤–ª—è–µ–º FPS –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                        cv2.putText(img, f'FPS: {avg_fps}', (5, 20),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2)

                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ Streamlit
                        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

                        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –∫–∞–¥—Ä
                        webcam_placeholder.image(img_rgb, channels="RGB", use_container_width=True)

                        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ç–µ–∫—É—â–∏–µ —ç–º–æ—Ü–∏–∏
                        if emotions:
                            emotion_text = "**Detected Emotions:**\n"
                            for i, (emotion, confidence) in enumerate(emotions):
                                emotion_text += f"Face {i + 1}: {emotion} ({confidence:.2f})\n"
                            emotions_placeholder.markdown(emotion_text)

                        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                        if emotion_history:
                            # –ü–æ–¥—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                            stats = {}
                            for emotion in emotion_history:
                                stats[emotion] = stats.get(emotion, 0) + 1

                            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                            stats_text = "**Recent Emotion Statistics:**\n"
                            for emotion, count in stats.items():
                                percent = (count / len(emotion_history)) * 100
                                stats_text += f"{emotion}: {percent:.1f}%\n"

                            stats_placeholder.markdown(stats_text)

                        fps_placeholder.metric("Current FPS", avg_fps)
                        start_time = current_time()

                except CaptureReadError as e:
                    st.error(f"Webcam error: {e}")
                except Exception as e:
                    st.error(f"Error in webcam processing: {e}")
                finally:
                    cap.release()
                    webcam_placeholder.empty()
                    emotions_placeholder.empty()
                    stats_placeholder.empty()
                    fps_placeholder.empty()

    with col2:
        # –°—Ç–∞—Ç—É—Å –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        st.markdown("#### üî¥ Live Status")
        if st.session_state.get('webcam_running', False):
            st.success("‚úÖ Webcam Active")
            st.info("Detecting faces and emotions in real-time")
        else:
            st.info("üì∑ Webcam Ready")

        st.markdown("---")

        # –¢–µ–∫—É—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        st.markdown("#### ‚öôÔ∏è Current Parameters")
        for key, value in st.session_state.backend_params.items():
            if key not in ['flip_h', 'show_preview']:
                st.metric(key.replace('_', ' ').title(), f"{value:.2f}" if isinstance(value, float) else value)

        st.markdown("---")

        # –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
        st.markdown("#### üìù Instructions")
        st.markdown("""
        1. Start webcam
        2. Look at the camera
        3. Emotions will be detected in real-time
        4. Adjust parameters in sidebar
        5. Stop when done
        """)


# ============================================
# –û–°–ù–û–í–ù–û–ô –ò–ù–¢–ï–†–§–ï–ô–°
# ============================================

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    display_header()
    display_sidebar()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –±—ç–∫–µ–Ω–¥–∞

    # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –≤ –≤–∫–ª–∞–¥–∫–∞—Ö
    tab1, tab2, tab3 = st.tabs(["üé¨ Upload Video", "üì∑ Webcam Live", "‚ùì Help & Support"])

    with tab1:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏
        if st.session_state.processing_status in ["idle", "starting"]:
            create_upload_section()

        if st.session_state.processing_status == "processing":
            process_video()
        elif st.session_state.processing_status == "completed":
            display_result()
        elif st.session_state.processing_status == "failed":
            display_error()

    with tab2:
        create_webcam_section()

    with tab3:
        st.markdown("### ‚ùì Frequently Asked Questions")

        faqs = [
            {
                "question": "How does real-time emotion detection work?",
                "answer": "The app uses DetectFaceAndRecognizeEmotion class which combines face detection and emotion recognition. It processes each video frame in real-time, drawing bounding boxes and emotion labels."
            },
            {
                "question": "What emotions can be detected?",
                "answer": "The system detects basic emotions: Happy, Sad, Angry, Surprise, Fear, Disgust, Neutral, and possibly others depending on the model."
            },
            {
                "question": "Do you store my videos or images?",
                "answer": "No. All processing is done locally. Videos are temporarily stored only during processing and deleted afterward."
            },
            {
                "question": "Can I adjust detection parameters?",
                "answer": "Yes! Use the sidebar to adjust parameters like detection confidence, window size for smoothing, and confidence thresholds."
            },
            {
                "question": "What if no faces are detected?",
                "answer": "Try adjusting the 'Min Detection Confidence' parameter in the sidebar. Also ensure faces are clearly visible and well-lit."
            },
            {
                "question": "Why is FPS displayed?",
                "answer": "FPS (Frames Per Second) shows the processing speed. Lower FPS means slower processing but might be more accurate."
            }
        ]

        for faq in faqs:
            with st.expander(f"**Q:** {faq['question']}"):
                st.markdown(f"**A:** {faq['answer']}")

        st.markdown("---")

        st.markdown("### üêõ Troubleshooting")

        issues = [
            ("Webcam not working", "Check browser permissions for camera access. Try refreshing the page."),
            ("No faces detected", "Adjust detection confidence parameter. Ensure good lighting."),
            ("Slow performance", "Try reducing video resolution or frame rate."),
            ("Import errors", "Ensure face_detection_and_emotion_recognition.py is in the current directory."),
            ("Low FPS", "The model might be computationally intensive. Try on a machine with GPU."),
        ]

        for issue, solution in issues:
            st.markdown(f"**{issue}:** {solution}")


# ============================================
# –ó–ê–ü–£–°–ö –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø
# ============================================

if __name__ == "__main__":
    try:
        main()

        # –û—á–∏—Å—Ç–∫–∞ –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
        atexit.register(lambda: st.session_state.get('detection_processor', EmotionDetectionProcessor()).reset())

    except Exception as e:
        st.error(f"Application error: {str(e)}")
        st.info("Please restart the application and try again.")

        if st.button("üîÑ Restart Application"):
            st.rerun()
